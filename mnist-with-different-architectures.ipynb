{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Day 11 Action item\n## I'll work today on classifying MNIST digits again but with different CNN acrhitectures."},{"metadata":{},"cell_type":"markdown","source":"### As usual we'll import used libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom keras.datasets import mnist\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom sklearn.model_selection import KFold\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,BatchNormalization\nfrom keras.optimizers import RMSprop,Adam,SGD\n%matplotlib inline","execution_count":44,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read the dataset from keras"},{"metadata":{"trusted":true},"cell_type":"code","source":"(x_train,y_train),(x_test,y_test) = mnist.load_data()\nprint('Train data shape, data={},labels={}'.format(x_train.shape,y_train.shape))\nprint('Test data shape, data={},labels={}'.format(x_test.shape,y_test.shape))","execution_count":2,"outputs":[{"output_type":"stream","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11493376/11490434 [==============================] - 0s 0us/step\nTrain data shape, data=(60000, 28, 28),labels=(60000,)\nTest data shape, data=(10000, 28, 28),labels=(10000,)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### preview some images"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axarr = plt.subplots(2,2)\naxarr[0,0].imshow(x_train[0])\naxarr[0,1].imshow(x_train[1])\naxarr[1,0].imshow(x_train[2])\naxarr[1,1].imshow(x_train[3])\nplt.show()","execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 4 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"### reshape dataset to have a single color channel \"gray scale\""},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.reshape((x_train.shape[0],28,28,1))\nx_test = x_test.reshape((x_test.shape[0],28,28,1))","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One hot encode the data, 10 classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train =to_categorical(y_train)\ny_test =to_categorical(y_test)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"(60000, 10)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.shape","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"(10000, 10)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### then we need to work on data types of pixels and normalize their values to be from 0 to 1 instead 0 to 255 for faster training"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train= x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train =x_train/255.0\nx_test = x_test/255.0","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.max()","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"1.0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.min()","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"0.0"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Building a simple model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = Sequential()\nmodel1.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\nmodel1.add(MaxPool2D((2, 2)))\nmodel1.add(Flatten())\nmodel1.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\nmodel1.add(Dense(10, activation='softmax'))\n# compile model\nopt = SGD(lr=0.01, momentum=0.9)\nmodel1.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":25,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### train the model and evaluate the scores."},{"metadata":{},"cell_type":"markdown","source":"### this code will be used in all the examples so i'll put it in a function"},{"metadata":{"trusted":true},"cell_type":"code","source":"scores, histories = [], []\n# prepare cross validation\nkfold = KFold(5, shuffle=True, random_state=1)\n# enumerate splits\nfor train_ix, test_ix in kfold.split(x_train):\n    # define model\n    # fit model\n    history = model1.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test), verbose=1)\n    # evaluate model\n    _, acc = model1.evaluate(x_test, y_test, verbose=1)\n    print('> %.3f' % (acc * 100.0))\n    # stores scores\n    scores.append(acc)\n    histories.append(history)","execution_count":32,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n469/469 [==============================] - 14s 30ms/step - loss: 8.6404e-04 - accuracy: 1.0000 - val_loss: 0.0407 - val_accuracy: 0.9889\nEpoch 2/10\n469/469 [==============================] - 13s 29ms/step - loss: 7.9989e-04 - accuracy: 1.0000 - val_loss: 0.0407 - val_accuracy: 0.9890\nEpoch 3/10\n469/469 [==============================] - 13s 29ms/step - loss: 7.6509e-04 - accuracy: 1.0000 - val_loss: 0.0412 - val_accuracy: 0.9889\nEpoch 4/10\n469/469 [==============================] - 13s 28ms/step - loss: 7.3618e-04 - accuracy: 1.0000 - val_loss: 0.0413 - val_accuracy: 0.9891\nEpoch 5/10\n469/469 [==============================] - 13s 29ms/step - loss: 7.0016e-04 - accuracy: 1.0000 - val_loss: 0.0420 - val_accuracy: 0.9890\nEpoch 6/10\n469/469 [==============================] - 13s 28ms/step - loss: 6.7685e-04 - accuracy: 1.0000 - val_loss: 0.0421 - val_accuracy: 0.9888\nEpoch 7/10\n469/469 [==============================] - 13s 28ms/step - loss: 6.5366e-04 - accuracy: 1.0000 - val_loss: 0.0421 - val_accuracy: 0.9886\nEpoch 8/10\n469/469 [==============================] - 13s 29ms/step - loss: 6.2763e-04 - accuracy: 1.0000 - val_loss: 0.0420 - val_accuracy: 0.9887\nEpoch 9/10\n469/469 [==============================] - 13s 28ms/step - loss: 5.9924e-04 - accuracy: 1.0000 - val_loss: 0.0422 - val_accuracy: 0.9888\nEpoch 10/10\n469/469 [==============================] - 13s 29ms/step - loss: 5.8492e-04 - accuracy: 1.0000 - val_loss: 0.0422 - val_accuracy: 0.9890\n313/313 [==============================] - 1s 4ms/step - loss: 0.0422 - accuracy: 0.9890\n> 98.900\nEpoch 1/10\n469/469 [==============================] - 13s 28ms/step - loss: 5.6476e-04 - accuracy: 1.0000 - val_loss: 0.0426 - val_accuracy: 0.9886\nEpoch 2/10\n469/469 [==============================] - 13s 28ms/step - loss: 5.4633e-04 - accuracy: 1.0000 - val_loss: 0.0428 - val_accuracy: 0.9890\nEpoch 3/10\n469/469 [==============================] - 14s 29ms/step - loss: 5.2976e-04 - accuracy: 1.0000 - val_loss: 0.0427 - val_accuracy: 0.9886\nEpoch 4/10\n469/469 [==============================] - 13s 28ms/step - loss: 5.1169e-04 - accuracy: 1.0000 - val_loss: 0.0433 - val_accuracy: 0.9887\nEpoch 5/10\n469/469 [==============================] - 13s 29ms/step - loss: 4.9997e-04 - accuracy: 1.0000 - val_loss: 0.0432 - val_accuracy: 0.9888\nEpoch 6/10\n469/469 [==============================] - 13s 28ms/step - loss: 4.8823e-04 - accuracy: 1.0000 - val_loss: 0.0433 - val_accuracy: 0.9889\nEpoch 7/10\n469/469 [==============================] - 13s 28ms/step - loss: 4.7030e-04 - accuracy: 1.0000 - val_loss: 0.0431 - val_accuracy: 0.9888\nEpoch 8/10\n469/469 [==============================] - 13s 29ms/step - loss: 4.5647e-04 - accuracy: 1.0000 - val_loss: 0.0434 - val_accuracy: 0.9887\nEpoch 9/10\n469/469 [==============================] - 13s 28ms/step - loss: 4.4655e-04 - accuracy: 1.0000 - val_loss: 0.0436 - val_accuracy: 0.9889\nEpoch 10/10\n469/469 [==============================] - 13s 29ms/step - loss: 4.3302e-04 - accuracy: 1.0000 - val_loss: 0.0440 - val_accuracy: 0.9889\n313/313 [==============================] - 1s 4ms/step - loss: 0.0440 - accuracy: 0.9889\n> 98.890\nEpoch 1/10\n469/469 [==============================] - 13s 28ms/step - loss: 4.2337e-04 - accuracy: 1.0000 - val_loss: 0.0447 - val_accuracy: 0.9885\nEpoch 2/10\n469/469 [==============================] - 13s 29ms/step - loss: 4.1354e-04 - accuracy: 1.0000 - val_loss: 0.0437 - val_accuracy: 0.9888\nEpoch 3/10\n469/469 [==============================] - 14s 30ms/step - loss: 4.0347e-04 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 0.9888\nEpoch 4/10\n469/469 [==============================] - 16s 34ms/step - loss: 3.9562e-04 - accuracy: 1.0000 - val_loss: 0.0443 - val_accuracy: 0.9888\nEpoch 5/10\n469/469 [==============================] - 13s 29ms/step - loss: 3.8352e-04 - accuracy: 1.0000 - val_loss: 0.0441 - val_accuracy: 0.9890\nEpoch 6/10\n469/469 [==============================] - 13s 28ms/step - loss: 3.7835e-04 - accuracy: 1.0000 - val_loss: 0.0447 - val_accuracy: 0.9887\nEpoch 7/10\n469/469 [==============================] - 13s 29ms/step - loss: 3.6499e-04 - accuracy: 1.0000 - val_loss: 0.0445 - val_accuracy: 0.9887\nEpoch 8/10\n469/469 [==============================] - 13s 28ms/step - loss: 3.6116e-04 - accuracy: 1.0000 - val_loss: 0.0446 - val_accuracy: 0.9889\nEpoch 9/10\n469/469 [==============================] - 13s 29ms/step - loss: 3.5176e-04 - accuracy: 1.0000 - val_loss: 0.0450 - val_accuracy: 0.9887\nEpoch 10/10\n469/469 [==============================] - 13s 29ms/step - loss: 3.4420e-04 - accuracy: 1.0000 - val_loss: 0.0448 - val_accuracy: 0.9888\n313/313 [==============================] - 1s 4ms/step - loss: 0.0448 - accuracy: 0.9888\n> 98.880\nEpoch 1/10\n469/469 [==============================] - 13s 28ms/step - loss: 3.3772e-04 - accuracy: 1.0000 - val_loss: 0.0449 - val_accuracy: 0.9886\nEpoch 2/10\n469/469 [==============================] - 14s 29ms/step - loss: 3.2967e-04 - accuracy: 1.0000 - val_loss: 0.0452 - val_accuracy: 0.9888\nEpoch 3/10\n469/469 [==============================] - 13s 28ms/step - loss: 3.2436e-04 - accuracy: 1.0000 - val_loss: 0.0453 - val_accuracy: 0.9887\nEpoch 4/10\n469/469 [==============================] - 14s 29ms/step - loss: 3.1606e-04 - accuracy: 1.0000 - val_loss: 0.0455 - val_accuracy: 0.9886\nEpoch 5/10\n469/469 [==============================] - 13s 28ms/step - loss: 3.1232e-04 - accuracy: 1.0000 - val_loss: 0.0458 - val_accuracy: 0.9885\nEpoch 6/10\n469/469 [==============================] - 13s 29ms/step - loss: 3.0543e-04 - accuracy: 1.0000 - val_loss: 0.0455 - val_accuracy: 0.9888\nEpoch 7/10\n469/469 [==============================] - 13s 29ms/step - loss: 2.9896e-04 - accuracy: 1.0000 - val_loss: 0.0456 - val_accuracy: 0.9886\nEpoch 8/10\n469/469 [==============================] - 13s 28ms/step - loss: 2.9284e-04 - accuracy: 1.0000 - val_loss: 0.0459 - val_accuracy: 0.9885\nEpoch 9/10\n469/469 [==============================] - 13s 29ms/step - loss: 2.8779e-04 - accuracy: 1.0000 - val_loss: 0.0457 - val_accuracy: 0.9889\nEpoch 10/10\n469/469 [==============================] - 13s 28ms/step - loss: 2.8269e-04 - accuracy: 1.0000 - val_loss: 0.0462 - val_accuracy: 0.9885\n313/313 [==============================] - 1s 4ms/step - loss: 0.0462 - accuracy: 0.9885\n> 98.850\nEpoch 1/10\n469/469 [==============================] - 14s 29ms/step - loss: 2.7802e-04 - accuracy: 1.0000 - val_loss: 0.0461 - val_accuracy: 0.9886\nEpoch 2/10\n469/469 [==============================] - 13s 28ms/step - loss: 2.7189e-04 - accuracy: 1.0000 - val_loss: 0.0462 - val_accuracy: 0.9886\nEpoch 3/10\n469/469 [==============================] - 13s 28ms/step - loss: 2.6933e-04 - accuracy: 1.0000 - val_loss: 0.0461 - val_accuracy: 0.9886\nEpoch 4/10\n469/469 [==============================] - 13s 29ms/step - loss: 2.6433e-04 - accuracy: 1.0000 - val_loss: 0.0462 - val_accuracy: 0.9887\nEpoch 5/10\n469/469 [==============================] - 13s 28ms/step - loss: 2.5791e-04 - accuracy: 1.0000 - val_loss: 0.0461 - val_accuracy: 0.9887\nEpoch 6/10\n469/469 [==============================] - 13s 29ms/step - loss: 2.5529e-04 - accuracy: 1.0000 - val_loss: 0.0462 - val_accuracy: 0.9887\nEpoch 7/10\n469/469 [==============================] - 13s 28ms/step - loss: 2.5088e-04 - accuracy: 1.0000 - val_loss: 0.0466 - val_accuracy: 0.9886\nEpoch 8/10\n469/469 [==============================] - 13s 28ms/step - loss: 2.4721e-04 - accuracy: 1.0000 - val_loss: 0.0467 - val_accuracy: 0.9886\nEpoch 9/10\n469/469 [==============================] - 13s 29ms/step - loss: 2.4103e-04 - accuracy: 1.0000 - val_loss: 0.0467 - val_accuracy: 0.9886\nEpoch 10/10\n469/469 [==============================] - 13s 28ms/step - loss: 2.3966e-04 - accuracy: 1.0000 - val_loss: 0.0465 - val_accuracy: 0.9888\n313/313 [==============================] - 1s 4ms/step - loss: 0.0465 - accuracy: 0.9888\n> 98.880\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### the results and accuracy of the 5 folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy: mean=%.3f std=%.3f, n=%d' % (np.mean(scores)*100, np.std(scores)*100, len(scores)))","execution_count":38,"outputs":[{"output_type":"stream","text":"Accuracy: mean=98.880 std=0.017, n=5\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Model 2 : adding batch normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def define_model_batchnorm():\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n    model.add(BatchNormalization())\n    model.add(Dense(10, activation='softmax'))\n    # compile model\n    opt = SGD(lr=0.01, momentum=0.9)\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 =define_model_batchnorm()","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores2, histories2 = [], []\n# prepare cross validation\nkfold = KFold(5, shuffle=True, random_state=1)\n# enumerate splits\nfor train_ix, test_ix in kfold.split(x_train):\n    # define model\n    # fit model\n    history = model2.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test), verbose=1)\n    # evaluate model\n    _, acc = model2.evaluate(x_test, y_test, verbose=1)\n    print('> %.3f' % (acc * 100.0))\n    # stores scores\n    scores2.append(acc)\n    histories2.append(history)","execution_count":49,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n469/469 [==============================] - 23s 48ms/step - loss: 0.2932 - accuracy: 0.9103 - val_loss: 0.0910 - val_accuracy: 0.9762\nEpoch 2/10\n469/469 [==============================] - 23s 48ms/step - loss: 0.0526 - accuracy: 0.9861 - val_loss: 0.0526 - val_accuracy: 0.9846\nEpoch 3/10\n469/469 [==============================] - 23s 48ms/step - loss: 0.0321 - accuracy: 0.9923 - val_loss: 0.0479 - val_accuracy: 0.9852\nEpoch 4/10\n469/469 [==============================] - 26s 57ms/step - loss: 0.0196 - accuracy: 0.9959 - val_loss: 0.0422 - val_accuracy: 0.9858\nEpoch 5/10\n469/469 [==============================] - 23s 49ms/step - loss: 0.0141 - accuracy: 0.9973 - val_loss: 0.0406 - val_accuracy: 0.9866\nEpoch 6/10\n469/469 [==============================] - 23s 49ms/step - loss: 0.0092 - accuracy: 0.9987 - val_loss: 0.0407 - val_accuracy: 0.9856\nEpoch 7/10\n469/469 [==============================] - 23s 48ms/step - loss: 0.0066 - accuracy: 0.9994 - val_loss: 0.0396 - val_accuracy: 0.9874\nEpoch 8/10\n469/469 [==============================] - 22s 48ms/step - loss: 0.0047 - accuracy: 0.9997 - val_loss: 0.0398 - val_accuracy: 0.9867\nEpoch 9/10\n469/469 [==============================] - 23s 49ms/step - loss: 0.0036 - accuracy: 0.9999 - val_loss: 0.0400 - val_accuracy: 0.9871\nEpoch 10/10\n469/469 [==============================] - 23s 48ms/step - loss: 0.0029 - accuracy: 0.9999 - val_loss: 0.0406 - val_accuracy: 0.9864\n313/313 [==============================] - 2s 5ms/step - loss: 0.0406 - accuracy: 0.9864\n> 98.640\nEpoch 1/10\n469/469 [==============================] - 23s 49ms/step - loss: 0.0022 - accuracy: 0.9999 - val_loss: 0.0403 - val_accuracy: 0.9870\nEpoch 2/10\n469/469 [==============================] - 23s 48ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0397 - val_accuracy: 0.9869\nEpoch 3/10\n469/469 [==============================] - 23s 48ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0399 - val_accuracy: 0.9875\nEpoch 4/10\n469/469 [==============================] - 23s 48ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0402 - val_accuracy: 0.9878\nEpoch 5/10\n469/469 [==============================] - 23s 48ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0411 - val_accuracy: 0.9876\nEpoch 6/10\n469/469 [==============================] - 23s 48ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0413 - val_accuracy: 0.9873\nEpoch 7/10\n469/469 [==============================] - 23s 48ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0414 - val_accuracy: 0.9872\nEpoch 8/10\n469/469 [==============================] - 22s 48ms/step - loss: 9.9100e-04 - accuracy: 1.0000 - val_loss: 0.0410 - val_accuracy: 0.9872\nEpoch 9/10\n469/469 [==============================] - 23s 49ms/step - loss: 9.2135e-04 - accuracy: 1.0000 - val_loss: 0.0410 - val_accuracy: 0.9873\nEpoch 10/10\n469/469 [==============================] - 23s 48ms/step - loss: 8.6568e-04 - accuracy: 1.0000 - val_loss: 0.0423 - val_accuracy: 0.9875\n313/313 [==============================] - 2s 5ms/step - loss: 0.0423 - accuracy: 0.9875\n> 98.750\nEpoch 1/10\n469/469 [==============================] - 23s 48ms/step - loss: 8.1435e-04 - accuracy: 1.0000 - val_loss: 0.0410 - val_accuracy: 0.9874\nEpoch 2/10\n469/469 [==============================] - 23s 49ms/step - loss: 7.5052e-04 - accuracy: 1.0000 - val_loss: 0.0414 - val_accuracy: 0.9876\nEpoch 3/10\n469/469 [==============================] - 23s 50ms/step - loss: 6.8188e-04 - accuracy: 1.0000 - val_loss: 0.0419 - val_accuracy: 0.9873\nEpoch 4/10\n469/469 [==============================] - 23s 48ms/step - loss: 6.5928e-04 - accuracy: 1.0000 - val_loss: 0.0415 - val_accuracy: 0.9871\nEpoch 5/10\n469/469 [==============================] - 23s 49ms/step - loss: 6.0868e-04 - accuracy: 1.0000 - val_loss: 0.0421 - val_accuracy: 0.9869\nEpoch 6/10\n469/469 [==============================] - 23s 49ms/step - loss: 5.4769e-04 - accuracy: 1.0000 - val_loss: 0.0420 - val_accuracy: 0.9877\nEpoch 7/10\n469/469 [==============================] - 23s 48ms/step - loss: 5.3896e-04 - accuracy: 1.0000 - val_loss: 0.0421 - val_accuracy: 0.9881\nEpoch 8/10\n469/469 [==============================] - 22s 48ms/step - loss: 5.1583e-04 - accuracy: 1.0000 - val_loss: 0.0427 - val_accuracy: 0.9877\nEpoch 9/10\n469/469 [==============================] - 23s 49ms/step - loss: 4.7209e-04 - accuracy: 1.0000 - val_loss: 0.0427 - val_accuracy: 0.9877\nEpoch 10/10\n469/469 [==============================] - 23s 50ms/step - loss: 4.7693e-04 - accuracy: 1.0000 - val_loss: 0.0421 - val_accuracy: 0.9874\n313/313 [==============================] - 2s 6ms/step - loss: 0.0421 - accuracy: 0.9874\n> 98.740\nEpoch 1/10\n469/469 [==============================] - 26s 55ms/step - loss: 4.6693e-04 - accuracy: 1.0000 - val_loss: 0.0418 - val_accuracy: 0.9877\nEpoch 2/10\n469/469 [==============================] - 23s 50ms/step - loss: 4.5108e-04 - accuracy: 1.0000 - val_loss: 0.0424 - val_accuracy: 0.9874\nEpoch 3/10\n469/469 [==============================] - 23s 49ms/step - loss: 4.0157e-04 - accuracy: 1.0000 - val_loss: 0.0428 - val_accuracy: 0.9878\nEpoch 4/10\n469/469 [==============================] - 23s 48ms/step - loss: 4.2307e-04 - accuracy: 1.0000 - val_loss: 0.0430 - val_accuracy: 0.9876\nEpoch 5/10\n469/469 [==============================] - 22s 48ms/step - loss: 3.8815e-04 - accuracy: 1.0000 - val_loss: 0.0431 - val_accuracy: 0.9873\nEpoch 6/10\n469/469 [==============================] - 23s 48ms/step - loss: 3.7358e-04 - accuracy: 1.0000 - val_loss: 0.0427 - val_accuracy: 0.9878\nEpoch 7/10\n469/469 [==============================] - 23s 48ms/step - loss: 3.6097e-04 - accuracy: 1.0000 - val_loss: 0.0428 - val_accuracy: 0.9879\nEpoch 8/10\n469/469 [==============================] - 23s 48ms/step - loss: 3.4630e-04 - accuracy: 1.0000 - val_loss: 0.0431 - val_accuracy: 0.9880\nEpoch 9/10\n469/469 [==============================] - 23s 48ms/step - loss: 3.4058e-04 - accuracy: 1.0000 - val_loss: 0.0432 - val_accuracy: 0.9878\nEpoch 10/10\n469/469 [==============================] - 23s 49ms/step - loss: 3.2194e-04 - accuracy: 1.0000 - val_loss: 0.0434 - val_accuracy: 0.9882\n313/313 [==============================] - 2s 5ms/step - loss: 0.0434 - accuracy: 0.9882\n> 98.820\nEpoch 1/10\n469/469 [==============================] - 23s 48ms/step - loss: 3.0978e-04 - accuracy: 1.0000 - val_loss: 0.0434 - val_accuracy: 0.9878\nEpoch 2/10\n469/469 [==============================] - 22s 47ms/step - loss: 3.0120e-04 - accuracy: 1.0000 - val_loss: 0.0432 - val_accuracy: 0.9886\nEpoch 3/10\n469/469 [==============================] - 23s 49ms/step - loss: 3.1432e-04 - accuracy: 1.0000 - val_loss: 0.0435 - val_accuracy: 0.9875\nEpoch 4/10\n469/469 [==============================] - 23s 48ms/step - loss: 2.9081e-04 - accuracy: 1.0000 - val_loss: 0.0434 - val_accuracy: 0.9876\nEpoch 5/10\n469/469 [==============================] - 22s 48ms/step - loss: 2.7679e-04 - accuracy: 1.0000 - val_loss: 0.0435 - val_accuracy: 0.9877\nEpoch 6/10\n469/469 [==============================] - 23s 49ms/step - loss: 2.6672e-04 - accuracy: 1.0000 - val_loss: 0.0440 - val_accuracy: 0.9876\nEpoch 7/10\n469/469 [==============================] - 23s 49ms/step - loss: 2.6112e-04 - accuracy: 1.0000 - val_loss: 0.0436 - val_accuracy: 0.9876\nEpoch 8/10\n469/469 [==============================] - 23s 48ms/step - loss: 2.5484e-04 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 0.9879\nEpoch 9/10\n469/469 [==============================] - 22s 48ms/step - loss: 2.3785e-04 - accuracy: 1.0000 - val_loss: 0.0440 - val_accuracy: 0.9881\nEpoch 10/10\n469/469 [==============================] - 23s 48ms/step - loss: 2.4435e-04 - accuracy: 1.0000 - val_loss: 0.0440 - val_accuracy: 0.9878\n313/313 [==============================] - 2s 5ms/step - loss: 0.0440 - accuracy: 0.9878\n> 98.780\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy: mean=%.3f std=%.3f, n=%d' % (np.mean(scores2)*100, np.std(scores2)*100, len(scores2)))","execution_count":51,"outputs":[{"output_type":"stream","text":"Accuracy: mean=98.746 std=0.060, n=5\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Model 3 : A deeper model inspired by VGG architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define cnn model\ndef define_model_d():\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n    model.add(MaxPool2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n    model.add(MaxPool2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(10, activation='softmax'))\n    # compile model\n    opt = SGD(lr=0.01, momentum=0.9)\n    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model3 =define_model_d()","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores3, histories3 = [], []\n# prepare cross validation\nkfold = KFold(5, shuffle=True, random_state=1)\n# enumerate splits\nfor train_ix, test_ix in kfold.split(x_train):\n    # define model\n    # fit model\n    history = model3.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test), verbose=1)\n    # evaluate model\n    _, acc = model3.evaluate(x_test, y_test, verbose=1)\n    print('> %.3f' % (acc * 100.0))\n    # stores scores\n    scores3.append(acc)\n    histories3.append(history)","execution_count":55,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n469/469 [==============================] - 38s 80ms/step - loss: 0.4418 - accuracy: 0.8559 - val_loss: 0.0633 - val_accuracy: 0.9792\nEpoch 2/10\n469/469 [==============================] - 37s 80ms/step - loss: 0.0634 - accuracy: 0.9798 - val_loss: 0.0438 - val_accuracy: 0.9857\nEpoch 3/10\n469/469 [==============================] - 37s 80ms/step - loss: 0.0429 - accuracy: 0.9860 - val_loss: 0.0355 - val_accuracy: 0.9888\nEpoch 4/10\n469/469 [==============================] - 38s 80ms/step - loss: 0.0343 - accuracy: 0.9893 - val_loss: 0.0334 - val_accuracy: 0.9894\nEpoch 5/10\n469/469 [==============================] - 38s 82ms/step - loss: 0.0260 - accuracy: 0.9917 - val_loss: 0.0318 - val_accuracy: 0.9905\nEpoch 6/10\n469/469 [==============================] - 38s 80ms/step - loss: 0.0219 - accuracy: 0.9933 - val_loss: 0.0314 - val_accuracy: 0.9897\nEpoch 7/10\n469/469 [==============================] - 38s 80ms/step - loss: 0.0181 - accuracy: 0.9944 - val_loss: 0.0300 - val_accuracy: 0.9894\nEpoch 8/10\n469/469 [==============================] - 38s 81ms/step - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.0288 - val_accuracy: 0.9913\nEpoch 9/10\n469/469 [==============================] - 38s 80ms/step - loss: 0.0126 - accuracy: 0.9963 - val_loss: 0.0317 - val_accuracy: 0.9897\nEpoch 10/10\n469/469 [==============================] - 38s 81ms/step - loss: 0.0111 - accuracy: 0.9966 - val_loss: 0.0272 - val_accuracy: 0.9913\n313/313 [==============================] - 2s 8ms/step - loss: 0.0272 - accuracy: 0.9913\n> 99.130\nEpoch 1/10\n469/469 [==============================] - 38s 81ms/step - loss: 0.0091 - accuracy: 0.9974 - val_loss: 0.0315 - val_accuracy: 0.9910\nEpoch 2/10\n469/469 [==============================] - 38s 81ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.0282 - val_accuracy: 0.9920\nEpoch 3/10\n469/469 [==============================] - 38s 81ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0288 - val_accuracy: 0.9919\nEpoch 4/10\n469/469 [==============================] - 37s 80ms/step - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.0279 - val_accuracy: 0.9924\nEpoch 5/10\n469/469 [==============================] - 38s 80ms/step - loss: 0.0041 - accuracy: 0.9989 - val_loss: 0.0305 - val_accuracy: 0.9913\nEpoch 6/10\n469/469 [==============================] - 38s 80ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0317 - val_accuracy: 0.9907\nEpoch 7/10\n469/469 [==============================] - 38s 80ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0307 - val_accuracy: 0.9920\nEpoch 8/10\n469/469 [==============================] - 38s 81ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0323 - val_accuracy: 0.9915\nEpoch 9/10\n469/469 [==============================] - 38s 80ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0311 - val_accuracy: 0.9925\nEpoch 10/10\n469/469 [==============================] - 37s 80ms/step - loss: 0.0010 - accuracy: 0.9998 - val_loss: 0.0333 - val_accuracy: 0.9924\n313/313 [==============================] - 2s 8ms/step - loss: 0.0333 - accuracy: 0.9924\n> 99.240\nEpoch 1/10\n469/469 [==============================] - 38s 82ms/step - loss: 6.6768e-04 - accuracy: 0.9999 - val_loss: 0.0327 - val_accuracy: 0.9921\nEpoch 2/10\n469/469 [==============================] - 37s 80ms/step - loss: 4.9915e-04 - accuracy: 1.0000 - val_loss: 0.0331 - val_accuracy: 0.9926\nEpoch 3/10\n469/469 [==============================] - 38s 80ms/step - loss: 5.0610e-04 - accuracy: 0.9999 - val_loss: 0.0331 - val_accuracy: 0.9926\nEpoch 4/10\n469/469 [==============================] - 38s 80ms/step - loss: 3.5273e-04 - accuracy: 1.0000 - val_loss: 0.0335 - val_accuracy: 0.9926\nEpoch 5/10\n469/469 [==============================] - 38s 80ms/step - loss: 3.4173e-04 - accuracy: 1.0000 - val_loss: 0.0340 - val_accuracy: 0.9927\nEpoch 6/10\n469/469 [==============================] - 38s 81ms/step - loss: 2.6392e-04 - accuracy: 1.0000 - val_loss: 0.0345 - val_accuracy: 0.9925\nEpoch 7/10\n469/469 [==============================] - 37s 80ms/step - loss: 2.3402e-04 - accuracy: 1.0000 - val_loss: 0.0346 - val_accuracy: 0.9924\nEpoch 8/10\n469/469 [==============================] - 38s 80ms/step - loss: 2.1452e-04 - accuracy: 1.0000 - val_loss: 0.0347 - val_accuracy: 0.9927\nEpoch 9/10\n469/469 [==============================] - 38s 80ms/step - loss: 1.9795e-04 - accuracy: 1.0000 - val_loss: 0.0345 - val_accuracy: 0.9929\nEpoch 10/10\n469/469 [==============================] - 38s 81ms/step - loss: 1.8096e-04 - accuracy: 1.0000 - val_loss: 0.0357 - val_accuracy: 0.9927\n313/313 [==============================] - 3s 9ms/step - loss: 0.0357 - accuracy: 0.9927\n> 99.270\nEpoch 1/10\n469/469 [==============================] - 40s 86ms/step - loss: 1.7560e-04 - accuracy: 1.0000 - val_loss: 0.0350 - val_accuracy: 0.9930\nEpoch 2/10\n469/469 [==============================] - 38s 81ms/step - loss: 1.6480e-04 - accuracy: 1.0000 - val_loss: 0.0360 - val_accuracy: 0.9925\nEpoch 3/10\n469/469 [==============================] - 37s 80ms/step - loss: 1.5111e-04 - accuracy: 1.0000 - val_loss: 0.0356 - val_accuracy: 0.9927\nEpoch 4/10\n469/469 [==============================] - 38s 80ms/step - loss: 1.4237e-04 - accuracy: 1.0000 - val_loss: 0.0360 - val_accuracy: 0.9925\nEpoch 5/10\n469/469 [==============================] - 38s 80ms/step - loss: 1.3182e-04 - accuracy: 1.0000 - val_loss: 0.0366 - val_accuracy: 0.9927\nEpoch 6/10\n469/469 [==============================] - 38s 80ms/step - loss: 1.2873e-04 - accuracy: 1.0000 - val_loss: 0.0366 - val_accuracy: 0.9928\nEpoch 7/10\n469/469 [==============================] - 38s 80ms/step - loss: 1.2356e-04 - accuracy: 1.0000 - val_loss: 0.0368 - val_accuracy: 0.9928\nEpoch 8/10\n469/469 [==============================] - 38s 81ms/step - loss: 1.1650e-04 - accuracy: 1.0000 - val_loss: 0.0371 - val_accuracy: 0.9925\nEpoch 9/10\n469/469 [==============================] - 38s 80ms/step - loss: 1.1027e-04 - accuracy: 1.0000 - val_loss: 0.0372 - val_accuracy: 0.9925\nEpoch 10/10\n469/469 [==============================] - 39s 83ms/step - loss: 1.0765e-04 - accuracy: 1.0000 - val_loss: 0.0376 - val_accuracy: 0.9926\n313/313 [==============================] - 2s 8ms/step - loss: 0.0376 - accuracy: 0.9926\n> 99.260\nEpoch 1/10\n469/469 [==============================] - 38s 80ms/step - loss: 1.0173e-04 - accuracy: 1.0000 - val_loss: 0.0377 - val_accuracy: 0.9925\nEpoch 2/10\n469/469 [==============================] - 38s 80ms/step - loss: 9.9357e-05 - accuracy: 1.0000 - val_loss: 0.0379 - val_accuracy: 0.9926\nEpoch 3/10\n469/469 [==============================] - 38s 80ms/step - loss: 9.4298e-05 - accuracy: 1.0000 - val_loss: 0.0382 - val_accuracy: 0.9924\nEpoch 4/10\n469/469 [==============================] - 38s 80ms/step - loss: 9.2398e-05 - accuracy: 1.0000 - val_loss: 0.0384 - val_accuracy: 0.9925\nEpoch 5/10\n469/469 [==============================] - 38s 80ms/step - loss: 8.8999e-05 - accuracy: 1.0000 - val_loss: 0.0381 - val_accuracy: 0.9925\nEpoch 6/10\n469/469 [==============================] - 38s 82ms/step - loss: 8.3676e-05 - accuracy: 1.0000 - val_loss: 0.0384 - val_accuracy: 0.9926\nEpoch 7/10\n469/469 [==============================] - 44s 94ms/step - loss: 8.2254e-05 - accuracy: 1.0000 - val_loss: 0.0387 - val_accuracy: 0.9924\nEpoch 8/10\n469/469 [==============================] - 39s 83ms/step - loss: 8.0150e-05 - accuracy: 1.0000 - val_loss: 0.0386 - val_accuracy: 0.9923\nEpoch 9/10\n469/469 [==============================] - 38s 82ms/step - loss: 7.7036e-05 - accuracy: 1.0000 - val_loss: 0.0387 - val_accuracy: 0.9927\nEpoch 10/10\n469/469 [==============================] - 39s 84ms/step - loss: 7.4685e-05 - accuracy: 1.0000 - val_loss: 0.0389 - val_accuracy: 0.9925\n313/313 [==============================] - 3s 8ms/step - loss: 0.0389 - accuracy: 0.9925\n> 99.250\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy: mean=%.3f std=%.3f, n=%d' % (np.mean(scores3)*100, np.std(scores3)*100, len(scores3)))","execution_count":56,"outputs":[{"output_type":"stream","text":"Accuracy: mean=99.230 std=0.051, n=5\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### that's it for today's action item ^^"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}